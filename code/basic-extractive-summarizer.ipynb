{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Functions Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_PATH = '../glove.6B.200d.pkl'\n",
    "ARTICLES_PATH = '../articles/'\n",
    "GOLD_SUMMARIES_PATH = '../gold_summaries/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads the global vector of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(VECTOR_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method used to split texts to sentences. Handles some of the special cases which I noticed and researched about. And returns a list of sentensec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"([0-9])\"\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text) \n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method reads the files which are in article path. This documents are splitted into sentences. The returned list contans two elements. First is document name and the second one is list of sentences.\n",
    "\n",
    "Takses at most 30 secs if you give same number of documents in article path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def parse_documents_to_sentences():\n",
    "    docs_by_sentences = []\n",
    "    doc_dirs = os.listdir(ARTICLES_PATH)\n",
    "    for f in doc_dirs:\n",
    "        fin = open(ARTICLES_PATH + f, 'r', encoding=\"latin-1\")\n",
    "        title = fin.readline() + \".\"\n",
    "        title = split_into_sentences(title)\n",
    "        doc = fin.read()\n",
    "        doc = split_into_sentences(doc)\n",
    "        docs_by_sentences.append([f, title + doc])\n",
    "    return docs_by_sentences\n",
    "\n",
    "parsed_docs = parse_documents_to_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence to vector method. Takes documents as sentences list(doc-name, list-of-sentences). And it firstly do case foldig for each sentence and parses the words. After that for a sentence it averages word vectors for each sentence. And this will be used as vector representation of sentence. Note that, if the word is not in the global vector it just skips that word.\n",
    "\n",
    "Takes at most 30 secs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sent2vec(docs_by_sentences):\n",
    "    docs_as_vectors = []\n",
    "    # SOME CODE HERE\n",
    "    for doc in docs_by_sentences:\n",
    "        docName = doc[0]\n",
    "        sentences = doc[1]\n",
    "        sentenceVectors = []\n",
    "        for s in sentences:\n",
    "            words = re.findall(r'\\w+', s.lower())\n",
    "            word_wectors = []\n",
    "            for w in words:\n",
    "                if w not in data:\n",
    "                    continue\n",
    "                word_wectors.append(data[w])\n",
    "            if(len(word_wectors) != 0):\n",
    "                sentenceVector = np.mean(np.array(word_wectors), axis=0)\n",
    "                sentenceVectors.append([s,sentenceVector])\n",
    "        docs_as_vectors.append([docName,sentenceVectors])    \n",
    "    return docs_as_vectors\n",
    "docs_as_vectors = np.array(sent2vec(parsed_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The clustring methods\n",
    "The training part of the k-means, it takes sentences list and cluster number as k. As return, it returns clusters of trained sentences, and means of those clusters. We will use this means to fit the test data. It selects first means random sentences. It is not complicated it just loops until means are not changed. If you want to run this method seperately you need to give a list of sentences and vectors of those sentences list, for example [ [ [ \"sentence\",   [vector of that sentence] ],  ...]\n",
    "\n",
    "This take the most time, in my local the maximum run time for only one call of this method takes at most 3 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_train(sentences, k):\n",
    "    means = []\n",
    "    for i in range(k):\n",
    "        means.append(np.random.choice(np.array(sentences)[:,1]))\n",
    "    while True:\n",
    "        clusters = [[] for i in range(k)]\n",
    "        for s in sentences:\n",
    "            sentence = s[0]\n",
    "            vector = s[1]\n",
    "            closest_cluster_index = np.argmin([np.linalg.norm(vector - mean) ** 2 for mean in means])\n",
    "            clusters[closest_cluster_index].append(s)\n",
    "        new_cluster_means = [[] for i in range(k)]\n",
    "        for i in range(len(means)):\n",
    "            cluster_total = []\n",
    "            for sent in clusters[i]:\n",
    "                cluster_total.append(sent[1])\n",
    "            if len(clusters[i]) == 0:\n",
    "                new_cluster_means[i] = means[i]\n",
    "            else:\n",
    "                new_cluster_means[i] = np.sum(np.array(cluster_total),axis=0) / len(clusters[i])\n",
    "        if np.array_equal(new_cluster_means, means):\n",
    "            break\n",
    "        means = new_cluster_means\n",
    "    return clusters, means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster fit method. This method takes docs as vectors like above return of sent2vec and means of the trained model. \n",
    "It just looks for the closest cluster mean and assigns this sentence to that cluster. After fitting to clusters, method chooses\n",
    "the most representative sentences as the nearest sentence to mean. Returns list of [docName, summary] pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_fit(docs_as_vectors, means):\n",
    "    extracted_summaries = []\n",
    "    for doc in docs_as_vectors:\n",
    "        docName = doc[0]\n",
    "        sentences = doc[1]\n",
    "        clusters = [[] for i in range(len(means))]\n",
    "        # fitting part\n",
    "        for s in sentences:\n",
    "            sentence = s[0]\n",
    "            vector = s[1]\n",
    "            closest_cluster_index = np.argmin([np.linalg.norm(vector - mean) ** 2 for mean in means])\n",
    "            clusters[closest_cluster_index].append(s)\n",
    "            \n",
    "        #summary part\n",
    "        extracted_summary = \"\"\n",
    "        for i in range(len(means)):\n",
    "            if len(clusters[i]) == 0:\n",
    "                continue\n",
    "            closest_index = np.argmin([np.linalg.norm(s[1] - means[i]) ** 2 for s in clusters[i]])\n",
    "            extracted_summary = extracted_summary + \" \" + clusters[i][closest_index][0]\n",
    "        extracted_summaries.append([docName,extracted_summary])\n",
    "    return extracted_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates the rouge scores, it expects only extracted_summaries. It assumes the golden summaries is named same as the articles. Reads the gold summaries and calculates rouge scores thanks to rouge library. Returns the rouge scores as extended list of those scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "def evaluation(extracted_summaries):\n",
    "    rouge_scores = []\n",
    "    for doc in extracted_summaries:\n",
    "        docName = doc[0]\n",
    "        summary = doc[1]\n",
    "        infile = open(GOLD_SUMMARIES_PATH+docName , 'r', encoding=\"latin-1\")\n",
    "        gold_summary = infile.read()\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(summary, gold_summary)\n",
    "        rouge_scores = rouge_scores + scores\n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the means and standard deviations of the rouge scores. Returns rouge score means and stds as list for each score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mean_var(scores):\n",
    "    mean1 = np.mean([ s['rouge-1']['f'] for s in scores])\n",
    "    std1 = np.std([ s['rouge-1']['f'] for s in scores])\n",
    "    mean2 = np.mean([ s['rouge-2']['f'] for s in scores])\n",
    "    std2 = np.std([ s['rouge-2']['f'] for s in scores])\n",
    "    meanl = np.mean([ s['rouge-l']['f'] for s in scores])\n",
    "    stdl = np.std([ s['rouge-l']['f'] for s in scores])\n",
    "    return [[mean1, std1], [mean2, std2], [meanl, stdl]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K - folding Part\n",
    "In this part it partitions the the all docs to K fold, You can change the K value as you wanted. I choose 5 as standard.\n",
    "\n",
    "Takes about 2 secs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "docs = parse_documents_to_sentences()\n",
    "np.random.shuffle(docs)\n",
    "train_docs = docs[:int(len(docs)*0.8)]\n",
    "test_docs = docs[int(len(docs)*0.8):]\n",
    "\n",
    "K = 5\n",
    "folds = []\n",
    "for i in range(K):\n",
    "    if i == K-1:\n",
    "        folds.append(train_docs[int(i*len(train_docs)/K):])\n",
    "    else:\n",
    "        folds.append(train_docs[int(i*len(train_docs)/K):int((i+1)*len(train_docs)/K)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculations of the first model. The difference of the models is just the clustring number of them, in the first model the k is 5 and in the second model k is 3. They are parted to 2 cells to reduce the runtime of one cell. For k-fold it appends other folds than validation fold and trains those folds and after that it fits the model with validation fold. After that calculates rouge scores and appends to rouge list. \n",
    "\n",
    "\n",
    "It takes about 8 minutes for K = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouges1 = []\n",
    "for i in range(K):\n",
    "    val = []\n",
    "    trains = []\n",
    "    for k in range(K):\n",
    "        if i == k:\n",
    "            val.extend(folds[k])\n",
    "        else:\n",
    "            trains.extend(folds[k])\n",
    "            \n",
    "    trains_vectors = np.array(sent2vec(trains))\n",
    "    sentences = []\n",
    "    for l in np.array(trains_vectors)[:,1]:\n",
    "        sentences = sentences + l\n",
    "    clusters1, means1 = kmeans_train(sentences, 5)\n",
    "    extracted_summaries1 = cluster_fit(sent2vec(val), means1)\n",
    "    rouge_scores1 = evaluation(extracted_summaries1)\n",
    "    rouges1.extend(rouge_scores1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculations of the second model. It also takes same time as above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouges2 = []\n",
    "for i in range(K):\n",
    "    val = []\n",
    "    trains = []\n",
    "    for k in range(K):\n",
    "        if i == k:\n",
    "            val.extend(folds[k])\n",
    "        else:\n",
    "            trains.extend(folds[k])\n",
    "            \n",
    "    trains_vectors = np.array(sent2vec(trains))\n",
    "    sentences = []\n",
    "    for l in np.array(trains_vectors)[:,1]:\n",
    "        sentences = sentences + l\n",
    "    clusters2, means2 = kmeans_train(sentences, 3)\n",
    "    extracted_summaries2 = cluster_fit(sent2vec(val), means2)\n",
    "    rouge_scores2 = evaluation(extracted_summaries2)\n",
    "    rouges2.extend(rouge_scores2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation scores of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation scores for model 1 : \n",
      "Rouge 1:0.42598107701725674±0.16094879556529504, Rouge 2:0.2889821115675144±0.18860871675946708, Rouge L:0.3649079837571582±0.16739646523337046\n",
      "\n",
      "The validation scores for model 2 : \n",
      "Rouge 1:0.40155678639737297±0.16453773778710803, Rouge 2:0.2714417577069077±0.18997116398463357, Rouge L:0.3308415297873292±0.16516762458107073\n"
     ]
    }
   ],
   "source": [
    "val1_scores = mean_var(rouges1)\n",
    "val2_scores = mean_var(rouges2)\n",
    "print(\"The validation scores for model 1 : \\nRouge 1:{}±{}, Rouge 2:{}±{}, Rouge L:{}±{}\".format(val1_scores[0][0], val1_scores[0][1], val1_scores[1][0], val1_scores[1][1], val1_scores[2][0], val1_scores[2][1]))\n",
    "print()\n",
    "\n",
    "print(\"The validation scores for model 2 : \\nRouge 1:{}±{}, Rouge 2:{}±{}, Rouge L:{}±{}\".format(val2_scores[0][0], val2_scores[0][1], val2_scores[1][0], val2_scores[1][1], val2_scores[2][0], val2_scores[2][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and tests of the models with all the data \n",
    "\n",
    "each cell takes about 3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_vectors = np.array(sent2vec(train_docs))\n",
    "sentences = []\n",
    "for l in np.array(trains_vectors)[:,1]:\n",
    "    sentences = sentences + l\n",
    "clusters1, means1 = kmeans_train(sentences, 5)\n",
    "extracted_summaries1 = cluster_fit(sent2vec(test_docs), means1)\n",
    "rouge_score1 = evaluation(extracted_summaries1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters2, means2 = kmeans_train(sentences, 3)\n",
    "extracted_summaries2 = cluster_fit(sent2vec(test_docs), means2)\n",
    "rouge_score2 = evaluation(extracted_summaries2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test scores of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test score for model 1 : \n",
      "Rouge 1:0.42598107701725674, Rouge 2:0.2889821115675144, Rouge L:0.3649079837571582\n",
      "\n",
      "The test score for model 2 : \n",
      "Rouge 1:0.40155678639737297, Rouge 2:0.2714417577069077, Rouge L:0.3308415297873292\n"
     ]
    }
   ],
   "source": [
    "test1 = mean_var(rouge_score1)\n",
    "test2 = mean_var(rouge_score2)\n",
    "\n",
    "print(\"The test score for model 1 : \\nRouge 1:{}, Rouge 2:{}, Rouge L:{}\".format(val1_scores[0][0], val1_scores[1][0], val1_scores[2][0]))\n",
    "print()\n",
    "\n",
    "print(\"The test score for model 2 : \\nRouge 1:{}, Rouge 2:{}, Rouge L:{}\".format(val2_scores[0][0], val2_scores[1][0], val2_scores[2][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model gives better  scores compared to first model. The only difference between them are number of clusters. First one is with 5 clusters and second one is with 3 clusters. The more cluster means in the all data there are more topics than 3 because 5 clusters gives better scores. With this we can give better results for one topic. But the worst thing for these models are they depend on each document even though the most important document for one summary is the summirized document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
